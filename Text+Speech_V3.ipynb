{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to AI\n",
    "## Text and Speech Demos\n",
    "Talk with your group about what you think each line of code does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Performing Basic Frequency Analysis\n",
    "Let's start by using some very basic frequency analysis on a document to see if we can determine what the document is about  based on word frequency.\n",
    "\n",
    "### Load a Text Document\n",
    "Run the cell below to load the transcript of a Shark Tank pitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a Shark Tank pitch by Pick-up Pools\n",
    "doc1 = open(\"SharkTank.txt\", \"r\")\n",
    "\n",
    "# Read the document and print its contents\n",
    "doc1Txt = doc1.read()\n",
    "print(doc1Txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the Text\n",
    "\n",
    "Text includes a lot of punctuation, which we need to remove if we want to work only with the actual words.\n",
    "\n",
    "Run the cell below to strip all the punctuation from the text and convert the words to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# remove numeric digits\n",
    "txt = ''.join(c for c in doc1Txt if not c.isdigit())\n",
    "\n",
    "# remove punctuation and make lower case\n",
    "txt = ''.join(c for c in txt if c not in punctuation).lower()\n",
    "\n",
    "# print the normalized text\n",
    "print (txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Frequency Distribution\n",
    "Now let's tokenize the text (split it into individual words), and count the number of times each word occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Tokenize the text into individual words\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "\n",
    "# Get the frequency distribution of the words into a data frame\n",
    "fdist = FreqDist(words)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "print (count_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Word Frequency\n",
    "It's often easier to analyze frequency by creating a visualization, such as a Pareto chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort the data frame by frequency\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "\n",
    "# Display the top 60 words as a bar plot\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words and Visualize Frequency Distribution for the Remaining Words\n",
    "\n",
    "A large number of the words in the text are common words like \"the\" or \"and\". These \"stopwords\" add little in the way of semantic meaning to the text, and won't help us determine the subject matter - so run the cell below to remove them. Now that we've prepared the text, we can tokenize the string into a list of individual words, and then perform frequency analysis on those words based on how often they appear in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get standard stop words from NLTK\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Filter out the stop words\n",
    "txt = ' '.join([word for word in txt.split() if word not in (stopwords.words('english'))])\n",
    "\n",
    "# Get the frequency distribution of the remaining words\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "fdist = FreqDist(words)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "\n",
    "# Plot the frequency of the top 60 words\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remind ourselves of the first document\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Shark Tank Pick-up Pools Pitch\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(doc1Txt)\n",
    "\n",
    "# Load the movie script of the opening scene for Spider-Man: Into the Spider-Verse\n",
    "doc2 = open(\"Spiderman.txt\", \"r\")\n",
    "doc2Txt = doc2.read()\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Spider-Man: Into the Spider-Verse\")\n",
    "print(\"------------------------------------------------\")\n",
    "print (doc2Txt)\n",
    "from string import punctuation\n",
    "txt2 = ''.join(c for c in doc2Txt if not c.isdigit())\n",
    "txt2 = ''.join(c for c in txt2 if c not in punctuation).lower()\n",
    "txt2 = ' '.join([word for word in txt2.split() if word not in (stopwords.words('english'))])\n",
    "\n",
    "\n",
    "# Load the play-by-play transcript of the Dallas Mavericks' Luka Doncic hitting a three pointer at the buzzer to win Game 4 of the NBA Playoffs against the Clippers\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"NBA Playoffs Luka Doncic Game Winner - Play by Play\")\n",
    "print(\"------------------------------------------------\")\n",
    "doc3 = open(\"PlayByPlay.txt\", \"r\")\n",
    "doc3Txt = doc3.read()\n",
    "print (doc3Txt)\n",
    "from string import punctuation\n",
    "txt3 = ''.join(c for c in doc3Txt if not c.isdigit())\n",
    "txt3 = ''.join(c for c in txt3 if c not in punctuation).lower()\n",
    "txt3 = ' '.join([word for word in txt3.split() if word not in (stopwords.words('english'))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Dig Deeper - TF-IDF\n",
    "#### Get TF-IDF Values for the top three words in each document\n",
    "\n",
    "In the previous example, we've used basic term frequency to determine each word's \"importance\" based on how often it appears in the document. When dealing with a large corpus of multiple documents, a more commonly used technique is *term frequency, inverse document frequency* (or TF-IDF) in which a score is calculated based on how often a word or term appears in one document compared to its more general frequency across the entire collection of documents. Using this technique, a high degree of relevance is assumed for words that appear frequently in a particular document, but relatively infrequently across a wide range of other documents. If this is confusing, don't worry! Run the examples below and it will become more clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install textblob library and define functions for TF-IDF\n",
    "!pip install -U textblob==0.10.0\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, doc):\n",
    "    return doc.words.count(word) / len(doc.words)\n",
    "\n",
    "def contains(word, docs):\n",
    "    return sum(1 for doc in docs if word in doc.words)\n",
    "\n",
    "def idf(word, docs):\n",
    "    return math.log(len(docs) / (1 + contains(word, docs)))\n",
    "\n",
    "def tfidf(word, doc, docs):\n",
    "    return tf(word,doc) * idf(word, docs)\n",
    "\n",
    "\n",
    "# Create a collection of documents as textblobs\n",
    "doc1 = tb(txt)\n",
    "doc2 = tb(txt2)\n",
    "doc3 = tb(txt3)\n",
    "docs = [doc1, doc2, doc3]\n",
    "\n",
    "# Use TF-IDF to get the three most important words from each document\n",
    "print('-----------------------------------------------------------')\n",
    "for i, doc in enumerate(docs):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, doc, docs) for word in doc.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "#### View frequency of unstemmed words from Lewis Capaldi's song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and print the lyrics to the song \"Someone You Loved\" by Lewis Capaldi\n",
    "doc4 = open(\"SomeoneYouLoved.txt\", \"r\")\n",
    "doc4Txt = doc4.read()\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Lyrics to 'Someone You Loved' by Lewis Capaldi\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(doc4Txt)\n",
    "\n",
    "# Normalize and remove stop words\n",
    "from string import punctuation\n",
    "doc4Txt = ''.join(c for c in doc4Txt if not c.isdigit())\n",
    "doc4Txt = ''.join(c for c in doc4Txt if c not in punctuation).lower()\n",
    "doc4Txt = ' '.join([word for word in doc4Txt.split() if word not in (stopwords.words('english'))])\n",
    "\n",
    "# Get Frequency distribution\n",
    "words = nltk.tokenize.word_tokenize(doc4Txt)\n",
    "fdist = FreqDist(words)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "\n",
    "# Plot frequency\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stem the words using the Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Get the word stems\n",
    "ps = PorterStemmer()\n",
    "stems = [ps.stem(word) for word in words]\n",
    "\n",
    "# Get Frequency distribution\n",
    "fdist = FreqDist(stems)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "\n",
    "# Plot frequency\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Named Entity Extraction using Spacy\n",
    "\n",
    "Named-entity recognition (NER) (also known as (named) entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. (Ref: Wikipedia)\n",
    "\n",
    "In this exercise we'll use a state-of-the-art text processing and NLP library called Spacy. You can tinker around with the text inputs, although you'll find that it works better on sentences where there are specific organizations, locations, names, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install these in binder \n",
    "\n",
    "! pip install -U pip setuptools wheel\n",
    "! pip install -U spacy\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\"\n",
    "\n",
    "doc1 = nlp(text1)\n",
    "displacy.render(doc1, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"Ben Solo was a human male Force-sensitive who fell to the dark side of the Force as Kylo Ren, master of the Knights of Ren and eventual Supreme Leader of the First Order, but returned to the light side shortly before his death. \"\n",
    "\n",
    "doc2 = nlp(text2)\n",
    "displacy.render(doc2, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"Hwang had conceived of the idea based on his own economic struggles early in life as well as the class disparity in South Korea and capitalism.\"\n",
    "\n",
    "doc3 = nlp(text3)\n",
    "displacy.render(doc3, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 Create a  Speech Service\n",
    "The Microsoft Cognitive Services include the Bing Speech service, that can interpret spoken input from a microphone or audio file. Follow these steps to provision the Bing speech service:\n",
    "\n",
    "1. Open another browser tab and navigate to https://portal.azure.com.\n",
    "2. Sign in using your Microsoft account.\n",
    "3. Click **+ Create a resource**, and search for \"**Speech**\".\n",
    "4. In the list of services, click ** Speech**.\n",
    "5. In the **Bing Speech** blade, click **Create**.\n",
    "6. In the **Create** blade, enter the following details, and then click **Create**\n",
    "  * **Name**: A unique name for your service.\n",
    "  * **Subscription**: Azure for Students or Microsoft Azure Sponsorship\n",
    "  * **Location**: **<u><font color=#ff0000>(US) South Central US</font></u>** **(If you do not select this location your code WILL NOT WORK!)**\n",
    "  * **Pricing tier**: Choose the Free F0 or Standard S0 pricing tier.\n",
    "  * **Resource Group**: Choose the existing resource group you used previously.\n",
    "7. Wait for the service to be created.\n",
    "8. When deployment is complete, click **Go to resource**.\n",
    "10. In the blade for your Speech service, click **Keys and Endpoint** and then copy **Key 1** to the clipboard and paste it into the **speechKey** variable assignment value in the cell below. \n",
    "11. Click the Run button to run the cell below that assigns the variable speechKey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "speechKey = 'YOUR_KEY_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the audio input\n",
    "We'll use a WAV file of captured audio containing the speech we want to transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "try:\n",
    "    speechKey\n",
    "except:\n",
    "    print(\"Make sure you run the code cell above that assigns the speechKey variable! Then run this cell again.\")\n",
    "    \n",
    "IPython.display.Audio('RainSpain.wav', autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Convert speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import requests, json\n",
    "\n",
    "speech_region = \"SouthCentralUS\"\n",
    "\n",
    "with open(\"RainSpain.wav\", mode=\"rb\") as audio_file:\n",
    "        audio_data =  audio_file.read()\n",
    "        \n",
    "# The Speech API requires an access token (valid for 10 mins)\n",
    "apiEndPoint = \"https://\" + speech_region + \".api.cognitive.microsoft.com/sts/v1.0/issueToken\"\n",
    "apiKey = speechKey\n",
    "headers = {\"Ocp-Apim-Subscription-Key\": apiKey}\n",
    "\n",
    "# Use the API key to request an access token\n",
    "response = requests.post(apiEndPoint, headers=headers)\n",
    "accesstoken = str(response.text)\n",
    "\n",
    "# Now that we have a token, we can set up the request\n",
    "speechToTextEndPoint = \"https://\" + speech_region + \".stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1\"\n",
    "headers = {\"Content-type\": \"audio/wav; codec=audio/pcm; samplerate=16000\", \n",
    "           \"Authorization\": \"Bearer \" + accesstoken}\n",
    "params = {\"language\":\"en-US\"}\n",
    "body = audio_data\n",
    "\n",
    "# Connect to server, post the request, and get the result\n",
    "response = requests.post(speechToTextEndPoint,data=body, params=params, headers=headers)\n",
    "result = str(response.text)\n",
    "print(json.loads(result)['DisplayText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 5 - NLP Algorithms using Google Colaboratory \n",
    "\n",
    "In these next exercises, you can use Google Colaboratory Notebooks online. Each Google Colaboratory (\"Colab\") notebook is its own virtual environment which can be run effortlessly in your browser. What you will do is click the link for `Open Google Colab Notebook` which will take you to the notebook located outside of Binder, somewhere on the internet. No installation on your computer is required. \n",
    "\n",
    "Each Google Colab notebook has been authored by different people over the years, and as such, are custom. We tried to select notebooks where the code is minimal and/or the explanations can be easily read. Don't get discouraged if some of the code and content is too complicated. Simply click each cell and wait for it to run. Be patient and don't click ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tips: \n",
    "You may come across these messages. Here are the ways you can solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run anyway**. You will see a pop-up that reads `Warning: This notebook was not authored by Google`. Click `Run anyway`.\n",
    "\n",
    "<img src=./img/runanyway.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change your runtime to GPU**. Google Colab offers free use of Tesla K80 GPUs. Go to the toolbar and select `Runtime` -> `Change Runtime Type`. Select `GPU` in the drop-down under `Hardware accelerator`.\n",
    "\n",
    "<img src=./img/gpu.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Too many sessions**. This means you have too many Google Colab notebooks running at the same time. Due to restricted, free, compute, you should only run one notebook (session at a time. \n",
    "\n",
    "<img src=./img/sessions.png>\n",
    "\n",
    "Click `Manage Sessions`. You can see in your Active Sessions all the notebooks you are simultaneously running. Click `TERMINATE` for all of them. Now `Close` and it returns you to your notebook. Click the last cell you left off at.\n",
    "\n",
    "<img src=./img/terminate.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Binary Text Classification for Sentiment Prediction\n",
    "\n",
    "A classic example of text classification is to use the IMDB Movie Review dataset which consists of 25,000 examples in the training set. You'll get to train a sequence2sequence model for 10 epochs. You'll also get to see some of the preprocessing you completed above in action used in the notebook like vectorization. Your model should be close to 87% accurate for sentiment prediction. There's alot of extra content in the cells. Feel free to read each if you like, since some of the cells include evaluating the model for accurate it is and visualizing the accuracy and loss per epoch you trained.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification.ipynb#scrollTo=QW355HH5L49K\">Open Google Colab Notebok</a>\n",
    "\n",
    "An example of the data is this review: \n",
    "\n",
    "> \"Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\"\n",
    "\n",
    "After training the model, you'll be able to enter some test sentences. The predicted outputs will be probabilities of how positive they are, such as 61.4% positive, 43.6% positive, or 35.3% positive.\n",
    "\n",
    "<img src=./img/predictions.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nice work!\n",
    "\n",
    "In this lab, you saw how to use many different Azure tools to process text and speech. There are so many applications you can create by combining these different tools. Brainstorm a few ideas you could build for the final project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Optional Post Camp Notebooks (Move on to the Final Project NOW!) \n",
    "\n",
    "### Text-to-Speech Transformer Model (Optional)\n",
    "This notebook conducts audio synthesis based on text input, using two types of algorithms: a Forward Transformer TTS and a WaveRNN Vocoder. Transformers are a special type of algorithm that are used for state-of-the-art NLP tasks today. This notebook loads pre-trained models that have been trained on extrenmely large datasets of language. Installing packages may take between 2 - 5 minutes to complete. After installation, all cells should run quickly. \n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/as-ideas/TransformerTTS/blob/master/notebooks/synthesize_forward_wavernn.ipynb#scrollTo=zdMgfG7GMF_R\"> Open Google Colab Notebook </a>\n",
    "\n",
    "Under `Synthesize text`, try entering your own custom sentence: \n",
    "`sentence = 'Scientists at the CERN laboratory, say they have discovered a new particle.'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question and Answering in Multiple Languages using BERT (Optional)\n",
    "BERT is a state-of-the-art language model. This notebook uses a pre-trained BERT model fine-tuned on multilingual Question and Answer data (Spanish, Russian, German, Hindi, Chinese). \n",
    "\n",
    "In each cell you will find the `context` of the question that is about to be asked. The `question` based on the `context` is then presented to the model. The BERT model then answers the question. In the last cell, you can create your own `context` and `question`. For example, my `context` was: \"The working hours of a normal person is 9 to 5\" / `question`: What are normal working hours? \n",
    "\n",
    "Try it out with another student in your class who might speak one of these languages. Ask them to enter a `context` and `question`.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/mrm8488/shared_colab_notebooks/blob/master/Try_mrm8488_xquad_finetuned_model.ipynb#scrollTo=WGA6yPiFe-JW\">Open Google Colab Notebook</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DialoGPT Chatbot (Optional) \n",
    "\n",
    "A transformer language model that was trained on 147M Reddit comment chains. You can create a chat bot in just 10 lines of code. \n",
    "\n",
    "<img src=./img/dialogpt.png>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/drive/1uib2-bxqYKR3lRdEmOh_MaWBGrMrPOUL#scrollTo=tDYhaGQJdJDr\">Open Google Colab Notebook</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis in Spanish (Optional)\n",
    "\n",
    "<a href=\"https://colab.research.google.com/drive/1ItS0-ZPXGcEeVmRmHaneX3w8eq6Vhdde?usp=sharing&fbclid=IwAR0z7XXLyy3KCJva9d-7zIXEYAZ8GK-9kXz4_hd_U_gfnu6-SoizgexOpXE#scrollTo=i7V1mxieDHII\"> Open Google Colab Notebook</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Generated Art using VQGAN + CLIP (Optional) \n",
    "\n",
    "**Note - this notebook takes a very long time to run since it is training a generative model.** It can take 25 minutes to generate images for 400 training iterations. \n",
    "\n",
    "Example Prompt: **\"sunny skies with girls on roller skates\"**\n",
    "\n",
    "<img src=./img/sunnygirl.png>\n",
    "\n",
    "A very user-friendly notebook that is self-explanatory and fun. Enter a prompt encoded by the CLIP algorithm and generate an image through VQGAN. Note, the setup and installation of libraries will take up to 5 minutes. Be patient!\n",
    "\n",
    "<a href=\"https://colab.research.google.com/drive/1n_xrgKDlGQcCF6O-eL3NOd_x4NSqAUjK#scrollTo=wSfISAhyPmyp\">Open Google Colab Notebook</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
