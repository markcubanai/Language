{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to AI\n",
    "## Text and Speech Demos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Basic Frequency Analysis\n",
    "Let's start by using some very basic frequency analysis on a document to see if we can determine what the document is about  based on word frequency.\n",
    "\n",
    "### Load a Text Document\n",
    "Run the cell below to load the transcript of a Shark Tank pitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a Shark Tank pitch by Pick-up Pools\n",
    "!wget -O SharkTank.txt https://raw.githubusercontent.com/markcubanai/Language/master/SharkTank.txt\n",
    "doc1 = open(\"SharkTank.txt\", \"r\")\n",
    "\n",
    "# Read the document and print its contents\n",
    "doc1Txt = doc1.read()\n",
    "print(doc1Txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the Text\n",
    "\n",
    "Text includes a lot of punctuation, which we need to remove if we want to work only with the actual words.\n",
    "\n",
    "Run the cell below to strip all the punctuation from the text and convert the words to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# remove numeric digits\n",
    "txt = ''.join(c for c in doc1Txt if not c.isdigit())\n",
    "\n",
    "# remove punctuation and make lower case\n",
    "txt = ''.join(c for c in txt if c not in punctuation).lower()\n",
    "\n",
    "# print the normalized text\n",
    "print (txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Frequency Distribution\n",
    "Now let's tokenize the text (split it into individual words), and count the number of times each word occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Tokenize the text into individual words\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "\n",
    "# Get the frequency distribution of the words into a data frame\n",
    "fdist = FreqDist(words)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "print (count_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Word Frequency\n",
    "It's often easier to analyze frequency by creating a visualization, such as a Pareto chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort the data frame by frequency\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "\n",
    "# Display the top 60 words as a bar plot\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words\n",
    "\n",
    "A large number of the words in the text are common words like \"the\" or \"and\". These \"stopwords\" add little in the way of semantic meaning to the text, and won't help us determine the subject matter - so run the cell below to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get standard stop words from NLTK\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Filter out the stop words\n",
    "txt = ' '.join([word for word in txt.split() if word not in (stopwords.words('english'))])\n",
    "\n",
    "# Get the frequency distribution of the remaining words\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "fdist = FreqDist(words)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "\n",
    "# Plot the frequency of the top 60 words\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Visualize the Frequency Distribution for the Remaining Words\n",
    "Now that we've prepared the text, we can tokenize the string into a list of individual words, and then perform frequency analysis on those words based on how often they appear in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remind ourselves of the first document\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Shark Tank Pick-up Pools Pitch\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(doc1Txt)\n",
    "\n",
    "# Load the movie script of the opening scene for Spider-Man: Into the Spider-Verse\n",
    "!wget -O Spiderman.txt https://raw.githubusercontent.com/markcubanai/Language/master/Spiderman.txt\n",
    "doc2 = open(\"Spiderman.txt\", \"r\")\n",
    "doc2Txt = doc2.read()\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Spider-Man: Into the Spider-Verse\")\n",
    "print(\"------------------------------------------------\")\n",
    "print (doc2Txt)\n",
    "from string import punctuation\n",
    "txt2 = ''.join(c for c in doc2Txt if not c.isdigit())\n",
    "txt2 = ''.join(c for c in txt2 if c not in punctuation).lower()\n",
    "txt2 = ' '.join([word for word in txt2.split() if word not in (stopwords.words('english'))])\n",
    "\n",
    "\n",
    "# Load the play-by-play transcript of the Dallas Mavericks' Luka Doncic hitting a three pointer at the buzzer to win Game 4 of the NBA Playoffs against the Clippers\n",
    "!wget -O PlayByPlay.txt https://raw.githubusercontent.com/markcubanai/Language/master/PlayByPlay.txt\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"NBA Playoffs Luka Doncic Game Winner - Play by Play\")\n",
    "print(\"------------------------------------------------\")\n",
    "doc3 = open(\"PlayByPlay.txt\", \"r\")\n",
    "doc3Txt = doc3.read()\n",
    "print (doc3Txt)\n",
    "from string import punctuation\n",
    "txt3 = ''.join(c for c in doc3Txt if not c.isdigit())\n",
    "txt3 = ''.join(c for c in txt3 if c not in punctuation).lower()\n",
    "txt3 = ' '.join([word for word in txt3.split() if word not in (stopwords.words('english'))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get TF-IDF Values for the top three words in each document\n",
    "\n",
    "In the previous example, we've used basic term frequency to determine each word's \"importance\" based on how often it appears in the document. When dealing with a large corpus of multiple documents, a more commonly used technique is *term frequency, inverse document frequency* (or TF-IDF) in which a score is calculated based on how often a word or term appears in one document compared to its more general frequency across the entire collection of documents. Using this technique, a high degree of relevance is assumed for words that appear frequently in a particular document, but relatively infrequently across a wide range of other documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install textblob library and define functions for TF-IDF\n",
    "!pip install -U textblob==0.10.0\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, doc):\n",
    "    return doc.words.count(word) / len(doc.words)\n",
    "\n",
    "def contains(word, docs):\n",
    "    return sum(1 for doc in docs if word in doc.words)\n",
    "\n",
    "def idf(word, docs):\n",
    "    return math.log(len(docs) / (1 + contains(word, docs)))\n",
    "\n",
    "def tfidf(word, doc, docs):\n",
    "    return tf(word,doc) * idf(word, docs)\n",
    "\n",
    "\n",
    "# Create a collection of documents as textblobs\n",
    "doc1 = tb(txt)\n",
    "doc2 = tb(txt2)\n",
    "doc3 = tb(txt3)\n",
    "docs = [doc1, doc2, doc3]\n",
    "\n",
    "# Use TF-IDF to get the three most important words from each document\n",
    "print('-----------------------------------------------------------')\n",
    "for i, doc in enumerate(docs):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, doc, docs) for word in doc.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "#### View frequency of unstemmed words from Lewis Capaldi's song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and print the lyrics to the song \"Someone You Loved\" by Lewis Capaldi\n",
    "!wget -O SomeoneYouLoved.txt https://raw.githubusercontent.com/markcubanai/Language/master/SomeoneYouLoved.txt\n",
    "doc4 = open(\"SomeoneYouLoved.txt\", \"r\")\n",
    "doc4Txt = doc4.read()\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Lyrics to 'Someone You Loved' by Lewis Capaldi\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(doc4Txt)\n",
    "\n",
    "# Normalize and remove stop words\n",
    "from string import punctuation\n",
    "doc4Txt = ''.join(c for c in doc4Txt if not c.isdigit())\n",
    "doc4Txt = ''.join(c for c in doc4Txt if c not in punctuation).lower()\n",
    "doc4Txt = ' '.join([word for word in doc4Txt.split() if word not in (stopwords.words('english'))])\n",
    "\n",
    "# Get Frequency distribution\n",
    "words = nltk.tokenize.word_tokenize(doc4Txt)\n",
    "fdist = FreqDist(words)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "\n",
    "# Plot frequency\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stem the words using the Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Get the word stems\n",
    "ps = PorterStemmer()\n",
    "stems = [ps.stem(word) for word in words]\n",
    "\n",
    "# Get Frequency distribution\n",
    "fdist = FreqDist(stems)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "\n",
    "# Plot frequency\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Service\n",
    "#### Create an Azure Cognitive Service for Language\n",
    "\n",
    "## Using the Language Service\n",
    "The previous examples demonstrate some ways to write code and analyze text, and they serve to illustrate that text analytics involves applying statistical techniques to text data in order to discern semantic meaning. This is a common theme in many AI solutions.\n",
    "\n",
    "Microsoft Cognitive Services includes a Language Service that encapsulates much more sophisticated techniques for ascertaining meaning from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Language Service\n",
    "To see this in action, you need to provision a Language service in your Azure subscription. Follow these steps to do that:\n",
    "\n",
    "1. Open another browser tab and navigate to https://portal.azure.com.\n",
    "2. Sign in using your Microsoft account.\n",
    "3. Click **+ Create a resource**, and search for \"**Language service**\".\n",
    "4. Click **Create**\n",
    "5. Select **Custom question answering**, then **Continue to create your resource**\n",
    "5. In the **Create Language** blade, enter the following details, and then click **Review + Create**\n",
    "  * **Subscription**: Azure for Students or Microsoft Azure Sponsorship\n",
    "  * **Resource Group**: Choose the existing resource group you created in the previous lab (or create a new one if you didn't complete the previous lab by clicking **Create new** and entering a unique name)\n",
    "  * **Region**: (US) South Central US\n",
    "  * **Name**: A unique name for your service.\n",
    "  * **Pricing tier**: Choose the Free F0 or Standard S pricing tier.\n",
    "7. When deployment is complete, click **Go to resource**.\n",
    "8. In the blade for your Language service, click **Keys and Endpoint** and then copy **Key 1** to the clipboard and paste it into the **textKey** variable assignment value in the cell below. \n",
    "9. Also copy the **Endpoint** to the clipboard and paste it into the **textAnalyticsEndpoint** variable assignment value in the cell below.\n",
    "10. Click the **Run** button to run the cell below that assigns the variables textKey and textAnalyticsEndpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textKey = 'YOUR_KEY_1'\n",
    "textAnalyticsEndpoint = 'https://YOUR_SERVICE_NAME.cognitiveservices.azure.com/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the Shark Tank Pitch and Game-Winner Play-By-Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import http.client, urllib.request, urllib.parse, urllib.error, base64, json, urllib\n",
    "\n",
    "    # Convert endpoint to URI\n",
    "    textAnalyticsURI = textAnalyticsEndpoint\n",
    "    if textAnalyticsURI.startswith('https://'):\n",
    "      textAnalyticsURI = textAnalyticsURI[len('https://'):]\n",
    "    while textAnalyticsURI.endswith('/'):\n",
    "      textAnalyticsURI = textAnalyticsURI[:-1]\n",
    "\n",
    "    # Define the request headers.\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Ocp-Apim-Subscription-Key': textKey,\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    # Define the parameters\n",
    "    params = urllib.parse.urlencode({\n",
    "    })\n",
    "\n",
    "    # Define the request body\n",
    "    body = {\n",
    "        \n",
    "    \"kind\": \"KeyPhraseExtraction\",\n",
    "    \"parameters\": {\n",
    "        \"modelVersion\": \"latest\"\n",
    "    },\n",
    "    \"analysisInput\":{\n",
    "        \"documents\":[\n",
    "        {\n",
    "            \"language\": \"en\",\n",
    "            \"id\": \"1\",\n",
    "            \"text\": doc1Txt\n",
    "        },\n",
    "        {\n",
    "            \"language\": \"en\",\n",
    "            \"id\": \"2\",\n",
    "            \"text\": doc3Txt\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Execute the REST API call and get the response.\n",
    "        conn = http.client.HTTPSConnection(textAnalyticsURI)\n",
    "        conn.request(\"POST\", \"/language/:analyze-text?api-version=2022-05-01\", str(body), headers)\n",
    "        response = conn.getresponse()\n",
    "        data = response.read().decode(\"UTF-8\")\n",
    "\n",
    "        # 'data' contains the JSON response, which includes a collection of documents.\n",
    "        parsed = json.loads(data)\n",
    "        print(parsed)\n",
    "        for document in parsed['results']['documents']:\n",
    "            print(\"Document \" + document[\"id\"] + \" key phrases:\")\n",
    "            for phrase in document['keyPhrases']:\n",
    "                print(\"  \" + phrase)\n",
    "            print(\"---------------------------\")\n",
    "        conn.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error:')\n",
    "        print(e)\n",
    "except:\n",
    "    print(\"Make sure you run the code cell above that assigns the textKey variable! Then run this cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Sentiment Analysis\n",
    "Another common AI requirement is to determine the sentiment associated with some text. For example, you might analyze tweets that include your organization's twitter handle to determine if they are positive or negative.\n",
    "\n",
    "Run the cell below to use the **sentiment** method of the Text Analytics service to discern the sentiment of two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"language\": \"en\",\n",
    "      \"id\": \"1\",\n",
    "      \"text\": \"Wow! cognitive services are fantastic.\"\n",
    "    },\n",
    "    {\n",
    "      \"language\": \"en\",\n",
    "      \"id\": \"2\",\n",
    "      \"text\": \"I hate it when computers don't understand me.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "try:\n",
    "    conn = http.client.HTTPSConnection(textAnalyticsURI)\n",
    "    conn.request(\"POST\", \"/text/analytics/v3.1/sentiment?%s\" % params, str(body), headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read().decode(\"UTF-8\")\n",
    "    parsed = json.loads(data)\n",
    "    \n",
    "    # Get the sentiment for each document\n",
    "    for document in parsed['documents']:\n",
    "        print(\"Document:\" + document[\"id\"] + \" = \" + max(document['confidenceScores']))\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Speech Service\n",
    "The Microsoft Cognitive Services include the Speech service, that can interpret spoken input from a microphone or audio file. Follow these steps to provision the Speech service:\n",
    "\n",
    "1. Open another browser tab and navigate to https://portal.azure.com.\n",
    "2. Sign in using your Microsoft account.\n",
    "3. Click **+ Create a resource**, and search for \"**Speech**\".\n",
    "4. In the list of services, click ** Speech**.\n",
    "5. In the **Speech** blade, enter the following details, and then click **Create**\n",
    "  * **Name**: A unique name for your service.\n",
    "  * **Subscription**: Azure for Students or Microsoft Azure Sponsorship\n",
    "  * **Location**: (US) South Central US\n",
    "  * **Pricing tier**: Choose the Free F0 or Standard S0 pricing tier.\n",
    "  * **Resource Group**: Choose the existing resource group you used previously.\n",
    "6. Wait for the service to be created.\n",
    "7. When deployment is complete, click **Go to resource**.\n",
    "8. In the blade for your Speech service, click **Keys and Endpoint** and then copy **Key 1** to the clipboard and paste it into the **speechKey** variable assignment value in the cell below. \n",
    "9. Paste the **Location/Region** value into the **speech_region** variable assignment value in the cell below.\n",
    "10. Click the Run button to run the cell below that assigns the variable speechKey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speechKey = 'YOUR_KEY_1'\n",
    "speech_region = 'YOUR_REGION'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the audio input\n",
    "We'll use a WAV file of captured audio containing the speech we want to transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "try:\n",
    "    speechKey\n",
    "except:\n",
    "    print(\"Make sure you run the code cell above that assigns the speechKey variable! Then run this cell again.\")\n",
    "\n",
    "!wget -O RainSpain.wav https://raw.githubusercontent.com/markcubanai/Language/master/RainSpain.wav\n",
    "IPython.display.Audio('RainSpain.wav', autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import requests, json\n",
    "\n",
    "with open(\"RainSpain.wav\", mode=\"rb\") as audio_file:\n",
    "        audio_data =  audio_file.read()\n",
    "        \n",
    "# The Speech API requires an access token (valid for 10 mins)\n",
    "apiEndPoint = \"https://\" + speech_region + \".api.cognitive.microsoft.com/sts/v1.0/issueToken\"\n",
    "apiKey = speechKey\n",
    "headers = {\"Ocp-Apim-Subscription-Key\": apiKey}\n",
    "\n",
    "# Use the API key to request an access token\n",
    "response = requests.post(apiEndPoint, headers=headers)\n",
    "accesstoken = str(response.text)\n",
    "\n",
    "# Now that we have a token, we can set up the request\n",
    "speechToTextEndPoint = \"https://\" + speech_region + \".stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1\"\n",
    "headers = {\"Content-type\": \"audio/wav; codec=audio/pcm; samplerate=16000\", \n",
    "           \"Authorization\": \"Bearer \" + accesstoken}\n",
    "params = {\"language\":\"en-US\"}\n",
    "body = audio_data\n",
    "\n",
    "# Connect to server, post the request, and get the result\n",
    "response = requests.post(speechToTextEndPoint,data=body, params=params, headers=headers)\n",
    "result = str(response.text)\n",
    "print(json.loads(result)['DisplayText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import requests, json\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "# Get the input text\n",
    "myText = input('What would you like me to say?: \\n')\n",
    "        \n",
    "# The Speech API requires an access token (valid for 10 mins)\n",
    "apiEndPoint = \"https://\" + speech_region + \".api.cognitive.microsoft.com/sts/v1.0/issueToken\"\n",
    "apiKey = speechKey\n",
    "headers = {\"Ocp-Apim-Subscription-Key\": apiKey}\n",
    "\n",
    "# Use the API key to request an access token\n",
    "response = requests.post(apiEndPoint, headers=headers)\n",
    "accesstoken = str(response.text)\n",
    "\n",
    "# Now that we have a token, we can set up the request\n",
    "textToSpeechEndPoint = \"https://\" + speech_region + \".tts.speech.microsoft.com/cognitiveservices/v1\"\n",
    "headers = {\"Content-type\": \"application/ssml+xml\",\n",
    "           'X-Microsoft-OutputFormat': 'riff-16khz-16bit-mono-pcm',\n",
    "           'User-Agent': 'speech',\n",
    "           \"Authorization\": \"Bearer \" + accesstoken}\n",
    "\n",
    "# The request body is XML\n",
    "body = \"<speak version='1.0' xml:lang='en-US'>\\\n",
    "          <voice xml:lang='en-US'\\\n",
    "                 xml:gender='Female'\\\n",
    "                 name='en-US-JennyNeural'>\" + myText + \"</voice>\\\n",
    "        </speak>\"\n",
    "\n",
    "\n",
    "# Connect to server, post the request, and get the result\n",
    "response = requests.post(textToSpeechEndPoint,data=body, headers=headers)\n",
    "\n",
    "#Play the audio\n",
    "IPython.display.Audio(response.content, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation\n",
    "\n",
    "### Create a Translator Service\n",
    "The Microsoft Cognitive Services include the Translator service, that can translate text between different languages. Follow these steps to provision the Translator service:\n",
    "\n",
    "1. Open another browser tab and navigate to https://portal.azure.com.\n",
    "2. Sign in using your Microsoft account.\n",
    "3. Click **+ Create a resource**, and search for \"**Translator**\".\n",
    "4. In the list of services, click **Translator**.\n",
    "5. In the **Translator** blade, click **Create**.\n",
    "6. In the **Create** blade, enter the following details, and then click **Review + Create**\n",
    "  * **Subscription**: Azure for Students or Microsoft Azure Sponsorship\n",
    "  * **Resource Group**: Choose the existing resource group you used previously.\n",
    "  * **Region**: Global\n",
    "  * **Name**: A unique name for your service.\n",
    "  * **Pricing tier**: Choose the Free F0 or Standard S1 pricing tier.\n",
    "7. Click **Review + Create**, then click **Create**\n",
    "8. Wait for the service to be created.\n",
    "9. When deployment is complete, click **Go to resource**.\n",
    "10. In the blade for your Translator Text service, click **Keys and Endpoint** and then copy **Key 1** to the clipboard and paste it into the **transTextKey** variable assignment value in the cell below. \n",
    "11. Click the **Run** button to run the cell below that assigns the variable transTextKey.\n",
    "\n",
    "#### Get the service key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transTextKey = \"YOUR_KEY_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate Text\n",
    "When asked to enter a language, note that you must enter the language code (i.e. \"en\" for English). Find all compatible language codes here: https://docs.microsoft.com/en-us/azure/cognitive-services/translator/language-support\n",
    "\n",
    "Sample Language Codes:\n",
    "\n",
    " * **English:** en\n",
    " * **Spanish:** es\n",
    " * **French:** fr\n",
    " * **German:** de\n",
    " * **Korean:** ko\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import requests, http.client, urllib.request, urllib.parse, urllib.error, base64, json, urllib\n",
    "    from xml.etree import ElementTree\n",
    "\n",
    "\n",
    "    textToTranslate = input('Please enter some text: \\n')\n",
    "    fromLangCode = input('What language is this? (Enter language code): \\n') \n",
    "    toLangCode = input('To what language would you like it translated? (Enter language code): \\n') \n",
    "\n",
    "    try:\n",
    "        # Define the request headers.\n",
    "        headers = {\n",
    "            'Ocp-Apim-Subscription-Key': transTextKey,\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        # Define the parameters\n",
    "        params = urllib.parse.urlencode({\n",
    "            \"api-version\": \"3.0\",\n",
    "            \"to\": toLangCode,\n",
    "            \"from\": fromLangCode\n",
    "        })\n",
    "\n",
    "        # Define the request body\n",
    "        requestBody = json.dumps([{\"Text\": textToTranslate}])\n",
    "\n",
    "        # Execute the REST API call and get the response.\n",
    "        conn = http.client.HTTPSConnection(\"api.cognitive.microsofttranslator.com\")\n",
    "        conn.request(\"POST\", \"/translate?%s\" % params, requestBody, headers)\n",
    "        response = conn.getresponse()\n",
    "        data = response.read().decode(\"UTF-8\")\n",
    "        parsed = json.loads(data)\n",
    "        print (parsed[0]['translations'][0]['text'])\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "except:\n",
    "    print(\"Make sure you run the code cell above that assigns the transTextKey variable! Then run this cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nice work!\n",
    "In this lab, you saw how to use many different Azure tools to process text and speech. There are so many applications you can create by combining these different tools. Brainstorm a few ideas you could build for the final project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
